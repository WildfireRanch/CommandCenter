# V1.7 Database Quality & Polling Validation

**Purpose:** Step-by-step validation of V1.7 Energy Analytics Dashboard data quality, database health, and polling mechanisms.

**Target:** Backend database, API endpoints, and data collection services

**Priority:** High - Ensure data accuracy before relying on analytics

---

## üéØ Objective

Validate that the V1.7 Energy Analytics Dashboard is built on a solid foundation:
1. Verify database schema integrity
2. Check data collection polling mechanisms
3. Validate data quality and accuracy
4. Test each new endpoint thoroughly
5. Identify any gaps in data collection

---

## üìã Validation Checklist

### PHASE 1: Database Schema Validation (15-20 min)

#### Step 1.1: Verify SolArk Schema
**What to check:**
- `solark.plant_flow` table exists and has data
- Check table structure and indexes
- Verify data retention period
- Check for missing or NULL values

**Commands:**
```bash
# Connect to database
railway run psql

# Check schema exists
\dn

# Check table structure
\d solark.plant_flow

# Check data count and date range
SELECT
    COUNT(*) as total_records,
    MIN(created_at) as oldest_record,
    MAX(created_at) as newest_record,
    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '24 hours') as records_last_24h,
    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '7 days') as records_last_7d
FROM solark.plant_flow;

# Check for data gaps in last 24 hours
SELECT
    DATE_TRUNC('hour', created_at) as hour,
    COUNT(*) as record_count,
    AVG(pv_power) as avg_solar,
    AVG(load_power) as avg_load,
    AVG(soc) as avg_soc
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '24 hours'
GROUP BY DATE_TRUNC('hour', created_at)
ORDER BY hour DESC;

# Check for NULL values
SELECT
    COUNT(*) FILTER (WHERE pv_power IS NULL) as null_pv_power,
    COUNT(*) FILTER (WHERE batt_power IS NULL) as null_batt_power,
    COUNT(*) FILTER (WHERE load_power IS NULL) as null_load_power,
    COUNT(*) FILTER (WHERE grid_power IS NULL) as null_grid_power,
    COUNT(*) FILTER (WHERE soc IS NULL) as null_soc
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '24 hours';
```

**Expected Results:**
- Table should exist in `solark` schema
- Should have data from last 24 hours minimum
- Records should be relatively continuous (not huge gaps)
- No excessive NULL values in critical columns

**Questions to Answer:**
- [ ] How many records exist in total?
- [ ] What's the oldest record date?
- [ ] How frequently is data being collected? (records per hour)
- [ ] Are there any significant gaps in data collection?
- [ ] What percentage of records have NULL values?

---

#### Step 1.2: Verify Victron Schema
**What to check:**
- `victron` schema exists
- `victron.battery_readings` table exists
- `victron.polling_status` table exists
- Check if Victron poller is running

**Commands:**
```bash
# Check if victron schema exists
SELECT schema_name
FROM information_schema.schemata
WHERE schema_name = 'victron';

# Check if tables exist
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'victron';

# If victron.battery_readings exists, check data
SELECT
    COUNT(*) as total_records,
    MIN(timestamp) as oldest_record,
    MAX(timestamp) as newest_record,
    COUNT(*) FILTER (WHERE timestamp >= NOW() - INTERVAL '24 hours') as records_last_24h
FROM victron.battery_readings;

# Check polling status (if table exists)
SELECT * FROM victron.polling_status WHERE id = 1;

# Check recent battery readings
SELECT
    timestamp,
    soc,
    voltage,
    current,
    power,
    temperature,
    state
FROM victron.battery_readings
ORDER BY timestamp DESC
LIMIT 10;
```

**Expected Results:**
- Schema may or may not exist (V1.7 works without it)
- If exists, should have recent data (< 3 minutes old)
- `polling_status` should show `is_healthy = true`

**Questions to Answer:**
- [ ] Does the Victron schema exist?
- [ ] If yes, how many battery readings exist?
- [ ] Is the Victron poller healthy and running?
- [ ] How frequently is Victron data collected?
- [ ] Are there discrepancies between Victron SOC and SolArk SOC?

**Action Items if Victron Missing:**
- [ ] Initialize Victron schema: `curl -X POST https://api.wildfireranch.us/db/init-schema`
- [ ] Check if Victron poller service is running
- [ ] Verify Victron API credentials are set
- [ ] Start Victron poller if not running

---

#### Step 1.3: Check TimescaleDB Configuration
**What to check:**
- TimescaleDB extension is enabled
- Hypertables are configured
- Retention policies are active

**Commands:**
```bash
# Check TimescaleDB extension
SELECT * FROM pg_extension WHERE extname = 'timescaledb';

# Check which tables are hypertables
SELECT
    hypertable_schema,
    hypertable_name,
    num_chunks,
    compression_enabled
FROM timescaledb_information.hypertables;

# Check retention policies
SELECT
    hypertable_name,
    proc_name,
    config,
    scheduled
FROM timescaledb_information.jobs
WHERE proc_name = 'policy_retention';

# Check data retention is working (solark)
SELECT
    DATE(created_at) as date,
    COUNT(*) as records
FROM solark.plant_flow
GROUP BY DATE(created_at)
ORDER BY date DESC
LIMIT 30;
```

**Expected Results:**
- TimescaleDB extension should be installed
- `solark.plant_flow` should be a hypertable
- `victron.battery_readings` should be a hypertable (if exists)
- Retention policy should be set for Victron (72 hours)

**Questions to Answer:**
- [ ] Is TimescaleDB properly configured?
- [ ] Are tables set up as hypertables for performance?
- [ ] Is data being automatically deleted per retention policies?
- [ ] How many days of SolArk data are being retained?

---

### PHASE 2: Data Collection Validation (20-30 min)

#### Step 2.1: SolArk Polling Mechanism
**What to check:**
- Identify how SolArk data is being collected
- Check polling frequency
- Validate data freshness
- Check for polling errors

**Commands:**
```bash
# Check latest SolArk data timestamp
SELECT
    MAX(created_at) as last_record,
    NOW() - MAX(created_at) as age_of_last_record,
    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '5 minutes') as records_last_5min
FROM solark.plant_flow;

# Check polling consistency over last hour
SELECT
    DATE_TRUNC('minute', created_at) as minute,
    COUNT(*) as record_count
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '1 hour'
GROUP BY DATE_TRUNC('minute', created_at)
ORDER BY minute DESC
LIMIT 60;

# Check for duplicate timestamps (shouldn't have many)
SELECT
    created_at,
    COUNT(*) as duplicate_count
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '24 hours'
GROUP BY created_at
HAVING COUNT(*) > 1
ORDER BY duplicate_count DESC
LIMIT 10;
```

**Check Backend Logs:**
```bash
# If running locally
cd railway
tail -f logs/app.log | grep -i "solark"

# Or check Railway deployment logs
# Look for patterns like:
# - "solark_poll_success"
# - "solark_poll_failed"
# - Error messages
```

**Questions to Answer:**
- [ ] How frequently is SolArk data collected? (every 30s? 1min? 5min?)
- [ ] When was the last successful poll?
- [ ] Are there consistent gaps in polling?
- [ ] What triggers SolArk data collection? (cron? webhook? API call?)
- [ ] Where is the SolArk poller code located?

**Files to Check:**
- Look for: `solark_poller.py`, `solark_scheduler.py`, or similar
- Check: `railway/src/services/`
- Check: `railway/src/integrations/`
- Check: Cron jobs or scheduled tasks

---

#### Step 2.2: Victron Polling Mechanism
**What to check:**
- Identify Victron poller service
- Check if it's running
- Validate polling frequency (should be ~3 minutes per API limits)
- Check for rate limiting issues

**Commands:**
```bash
# Check Victron health endpoint
curl -s "https://api.wildfireranch.us/victron/health" | jq

# Check latest Victron data
curl -s "https://api.wildfireranch.us/victron/battery/current" | jq

# Manually trigger a poll (for testing)
curl -X POST "https://api.wildfireranch.us/victron/poll-now" | jq

# Check database polling status
SELECT
    last_poll_attempt,
    last_successful_poll,
    consecutive_failures,
    is_healthy,
    requests_this_hour,
    last_error
FROM victron.polling_status
WHERE id = 1;
```

**Check for Poller Service:**
```bash
# Find Victron poller code
find railway/src -name "*victron*" -type f

# Check if poller is running as a service
# Look for background service or cron job
```

**Questions to Answer:**
- [ ] Is the Victron poller service running?
- [ ] When was the last successful poll?
- [ ] How many API requests have been made this hour? (limit: 50/hour)
- [ ] Are there consecutive failures?
- [ ] What's the last error message (if any)?
- [ ] Is the poller respecting rate limits?

**Expected Polling Pattern:**
- Should poll every ~3 minutes (20 times per hour)
- Should stay well under 50 requests/hour limit
- Should show `is_healthy: true` in polling_status

---

#### Step 2.3: Data Quality Checks
**What to check:**
- Validate data ranges make sense
- Check for anomalies
- Compare Victron vs SolArk SOC accuracy

**Commands:**
```bash
# Check for unrealistic values (last 24h)
SELECT
    MIN(pv_power) as min_solar,
    MAX(pv_power) as max_solar,
    MIN(soc) as min_soc,
    MAX(soc) as max_soc,
    MIN(load_power) as min_load,
    MAX(load_power) as max_load
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '24 hours';

# Check SOC comparison (if Victron exists)
SELECT
    s.created_at,
    s.soc as solark_soc,
    v.soc as victron_soc,
    ABS(s.soc - v.soc) as soc_difference
FROM solark.plant_flow s
LEFT JOIN LATERAL (
    SELECT soc, timestamp
    FROM victron.battery_readings
    WHERE timestamp BETWEEN s.created_at - INTERVAL '1 minute' AND s.created_at + INTERVAL '1 minute'
    ORDER BY ABS(EXTRACT(EPOCH FROM (timestamp - s.created_at)))
    LIMIT 1
) v ON true
WHERE s.created_at >= NOW() - INTERVAL '1 hour'
ORDER BY soc_difference DESC
LIMIT 10;

# Check for flatlined data (could indicate poller stuck)
SELECT
    pv_power,
    COUNT(*) as consecutive_count
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '1 hour'
GROUP BY pv_power
HAVING COUNT(*) > 10
ORDER BY consecutive_count DESC;
```

**Questions to Answer:**
- [ ] Are solar power values realistic? (0-10kW range typical)
- [ ] Are SOC values between 0-100%?
- [ ] Is battery power reasonable? (typical: -5kW to +5kW)
- [ ] Do Victron and SolArk SOC values align? (within 2-5% is normal)
- [ ] Are there flatlined values suggesting poller issues?

---

### PHASE 3: V1.7 Endpoint Testing (30-40 min)

#### Step 3.1: Test /energy/history Endpoint
**What to test:**
- Returns data for various time ranges
- Properly merges SolArk + Victron data
- Handles missing Victron data gracefully

**Test Cases:**
```bash
# Test 1: Basic query (1 hour, small limit)
curl -s "https://api.wildfireranch.us/energy/history?hours=1&limit=10" | jq '.status, .count, .data[0]'

# Expected: Should return merged data with timestamps, power values, and optionally Victron data

# Test 2: 24-hour history
curl -s "https://api.wildfireranch.us/energy/history?hours=24&limit=500" | jq '.status, .count'

# Expected: Should return ~288-1440 records depending on polling frequency

# Test 3: Extended range (7 days)
curl -s "https://api.wildfireranch.us/energy/history?hours=168&limit=2000" | jq '.status, .count'

# Expected: Should handle larger time ranges

# Test 4: Maximum range (30 days)
curl -s "https://api.wildfireranch.us/energy/history?hours=720&limit=5000" | jq '.status, .count'

# Expected: Should respect data retention limits

# Test 5: Check data structure
curl -s "https://api.wildfireranch.us/energy/history?hours=1&limit=1" | jq '.data[0]'

# Expected fields:
# - timestamp
# - pv_power
# - battery_power
# - soc
# - load_power
# - grid_power
# - victron_soc (optional)
# - voltage (optional)
# - current (optional)
# - temperature (optional)
```

**Validation Checklist:**
- [ ] Endpoint returns 200 OK status
- [ ] Data count matches expected range
- [ ] Timestamps are properly formatted (ISO 8601)
- [ ] All required fields are present
- [ ] Values are within realistic ranges
- [ ] Victron data is merged when available
- [ ] Works gracefully without Victron data

---

#### Step 3.2: Test /energy/analytics/daily Endpoint
**What to test:**
- Daily aggregations are accurate
- Excess energy calculations are correct
- Handles multiple time periods (7d, 30d, 365d)

**Test Cases:**
```bash
# Test 1: 7-day analytics
curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=7" | jq '.status, .count, .data[0]'

# Expected: Daily breakdown with excess energy calculations

# Test 2: 30-day analytics
curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=30" | jq '.status, .count'

# Expected: Should return up to 30 days of data

# Test 3: Validate excess energy calculation
curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=1" | jq '.data[0] | {
    total_solar_kwh,
    total_load_kwh,
    battery_charging_kwh,
    excess_energy_kwh,
    excess_energy_pct,
    excess_value_usd
}'

# Manually verify: excess = solar - (load + battery_charging)

# Test 4: Check all calculated metrics
curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=7" | jq '.data[0]'

# Expected fields:
# - date
# - total_solar_kwh
# - total_load_kwh
# - excess_energy_kwh
# - solar_self_consumption_pct
# - grid_independence_pct
# - avg_soc, min_soc, max_soc
```

**Manual Validation:**
Pick one day's data and manually verify calculations:
```bash
# Get raw data for a specific day
SELECT
    SUM(pv_power) / 60000.0 as total_solar_kwh,
    SUM(load_power) / 60000.0 as total_load_kwh,
    SUM(CASE WHEN batt_power > 0 THEN batt_power ELSE 0 END) / 60000.0 as battery_charging_kwh
FROM solark.plant_flow
WHERE DATE(created_at) = '2025-10-12';

# Calculate: excess = solar - (load + battery_charging)
# Compare with endpoint result
```

**Validation Checklist:**
- [ ] Daily aggregations match raw data
- [ ] Excess energy formula is correct
- [ ] Percentages add up correctly
- [ ] All kWh values are positive
- [ ] Date formatting is correct
- [ ] Returns data for requested number of days

---

#### Step 3.3: Test /energy/analytics/excess Endpoint
**What to test:**
- Excess power calculation in real-time
- Hourly patterns are meaningful
- Recommendations are sensible

**Test Cases:**
```bash
# Test 1: Last 24 hours excess analysis
curl -s "https://api.wildfireranch.us/energy/analytics/excess?hours=24" | jq '{
    status: .status,
    total_excess_kwh: .summary.total_excess_kwh,
    peak_excess_power_w: .summary.peak_excess_power_w,
    recommendations: .recommendations.suggested_actions | length
}'

# Test 2: Check time series data
curl -s "https://api.wildfireranch.us/energy/analytics/excess?hours=24" | jq '.time_series | length'

# Expected: Downsampled data points (every 5th record)

# Test 3: Validate hourly patterns
curl -s "https://api.wildfireranch.us/energy/analytics/excess?hours=24" | jq '.hourly_patterns'

# Expected: Object with hours 00-23 and average excess power

# Test 4: Check recommendations
curl -s "https://api.wildfireranch.us/energy/analytics/excess?hours=24" | jq '.recommendations.suggested_actions'

# Expected: Array of actionable recommendations based on excess energy levels

# Test 5: Verify excess calculation in database
```

**Manual Calculation Verification:**
```sql
-- Calculate excess power for a specific time
SELECT
    created_at,
    pv_power,
    load_power,
    batt_power,
    CASE
        WHEN pv_power > (load_power + CASE WHEN batt_power > 0 THEN batt_power ELSE 0 END)
        THEN pv_power - (load_power + CASE WHEN batt_power > 0 THEN batt_power ELSE 0 END)
        ELSE 0
    END as calculated_excess
FROM solark.plant_flow
WHERE created_at >= NOW() - INTERVAL '1 hour'
ORDER BY calculated_excess DESC
LIMIT 5;

-- Compare with endpoint's peak_excess_times
```

**Validation Checklist:**
- [ ] Excess formula: `Solar - (Load + BatteryCharging)` is correct
- [ ] Total excess kWh makes sense
- [ ] Peak excess times show highest values
- [ ] Hourly patterns show when most excess occurs
- [ ] Recommendations trigger at appropriate thresholds
- [ ] Potential value calculation is reasonable

---

#### Step 3.4: Test /energy/analytics/load-opportunities Endpoint
**What to test:**
- Real-time status is accurate
- Solar forecast is reasonable
- Opportunities trigger at correct thresholds

**Test Cases:**
```bash
# Test 1: Get current opportunities
curl -s "https://api.wildfireranch.us/energy/analytics/load-opportunities" | jq '{
    status: .status,
    current_excess: .current_status.excess_power_w,
    battery_soc: .current_status.battery_soc_pct,
    opportunities: .opportunities | length,
    immediate: .summary.immediate_actions,
    scheduled: .summary.scheduled_actions,
    warnings: .summary.warnings
}'

# Test 2: Check current status accuracy
curl -s "https://api.wildfireranch.us/energy/analytics/load-opportunities" | jq '.current_status'

# Compare with /energy/latest endpoint
curl -s "https://api.wildfireranch.us/energy/latest" | jq '.data | {pv_power, load_power, soc}'

# Values should match (within 1-2 minutes)

# Test 3: Verify solar forecast
curl -s "https://api.wildfireranch.us/energy/analytics/load-opportunities" | jq '.solar_forecast_6h'

# Expected: Next 6 hours with predicted solar based on historical averages

# Test 4: Check opportunity types
curl -s "https://api.wildfireranch.us/energy/analytics/load-opportunities" | jq '.opportunities[] | {type, load, action, priority}'

# Expected types: immediate, scheduled, warning

# Test 5: Validate opportunity logic
# If excess > 1000W ‚Üí Should suggest "Bitcoin Miners"
# If SOC > 80% AND solar > 3kW ‚Üí Should suggest "Irrigation Pumps"
# If excess > 500W AND SOC > 70% ‚Üí Should suggest "Water Heater"
# If SOC < 30% AND solar < 500W ‚Üí Should warn "STOP All Discretionary Loads"
```

**Validation Checklist:**
- [ ] Current status matches latest data
- [ ] Excess power calculation is accurate
- [ ] Solar forecast uses 7-day historical averages
- [ ] Opportunities trigger at correct thresholds
- [ ] Priority levels are appropriate
- [ ] Recommendations are actionable
- [ ] No duplicate opportunities

---

#### Step 3.5: Test /energy/analytics/cost Endpoint
**What to test:**
- Cost calculations are accurate
- Handles custom rates
- Date range filtering works

**Test Cases:**
```bash
# Test 1: Last 30 days with default rates
END_DATE=$(date +%Y-%m-%d)
START_DATE=$(date -d '30 days ago' +%Y-%m-%d)

curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=$START_DATE&end_date=$END_DATE" | jq '{
    period: .period,
    solar_produced: .energy.solar_produced_kwh,
    grid_import_cost: .costs.grid_import_cost,
    net_savings: .costs.net_savings,
    self_consumption: .metrics.solar_self_consumption_pct
}'

# Test 2: Custom electricity rates
curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=$START_DATE&end_date=$END_DATE&import_rate=0.15&export_rate=0.10" | jq '.costs'

# Test 3: Verify calculations
curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=$START_DATE&end_date=$END_DATE" | jq '{
    grid_import_kwh: .energy.grid_import_kwh,
    import_rate: .rates.import_rate_per_kwh,
    calculated_cost: (.energy.grid_import_kwh * .rates.import_rate_per_kwh),
    reported_cost: .costs.grid_import_cost
}'

# Calculated_cost should match reported_cost

# Test 4: Different date ranges
curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=2025-10-01&end_date=2025-10-07" | jq '.period'
```

**Manual Validation:**
```bash
# Compare with database query
railway run psql -c "
SELECT
    SUM(CASE WHEN grid_power > 0 THEN grid_power ELSE 0 END) / 60000.0 as grid_import_kwh,
    SUM(pv_power) / 60000.0 as solar_kwh
FROM solark.plant_flow
WHERE created_at >= '$START_DATE' AND created_at <= '$END_DATE';
"

# Calculate costs manually and compare
```

**Validation Checklist:**
- [ ] Cost calculations match manual verification
- [ ] Custom rates are applied correctly
- [ ] Solar savings calculation is accurate
- [ ] Net savings = solar_savings + export_revenue - import_cost
- [ ] Date range filtering works
- [ ] All monetary values are rounded to 2 decimals

---

#### Step 3.6: Test /energy/predictions/soc Endpoint
**What to test:**
- Predictions are reasonable
- Uses 7-day historical patterns
- Confidence levels are appropriate

**Test Cases:**
```bash
# Test 1: 24-hour prediction
curl -s "https://api.wildfireranch.us/energy/predictions/soc?hours=24" | jq '{
    status: .status,
    current_soc: .current_soc,
    prediction_hours: .prediction_hours,
    predictions_count: .predictions | length,
    model: .model
}'

# Test 2: Check prediction structure
curl -s "https://api.wildfireranch.us/energy/predictions/soc?hours=24" | jq '.predictions[0:5]'

# Expected fields: timestamp, hour, predicted_soc, confidence

# Test 3: Verify predictions are reasonable
curl -s "https://api.wildfireranch.us/energy/predictions/soc?hours=24" | jq '[.predictions[] | .predicted_soc] | min, max'

# SOC should stay between 0-100%

# Test 4: Check confidence distribution
curl -s "https://api.wildfireranch.us/energy/predictions/soc?hours=24" | jq '[.predictions[] | .confidence] | group_by(.) | map({confidence: .[0], count: length})'

# Test 5: 72-hour prediction (max)
curl -s "https://api.wildfireranch.us/energy/predictions/soc?hours=72" | jq '.predictions | length'

# Should return 72 predictions
```

**Validation Checklist:**
- [ ] Current SOC matches latest reading
- [ ] Predictions stay within 0-100% range
- [ ] Confidence levels vary based on data availability
- [ ] Historical patterns are being used
- [ ] Prediction count matches requested hours

---

### PHASE 4: Frontend Integration Testing (15-20 min)

#### Step 4.1: Test Dashboard Tabs
**What to test:**
- All 6 tabs load without errors
- Data is displayed correctly
- Charts render properly

**Test Cases:**
1. **Real-time Tab**
   - [ ] Power flow displays current values
   - [ ] Battery SOC shows accurate percentage
   - [ ] Victron vs SolArk comparison works (if Victron exists)
   - [ ] System insights show appropriate messages

2. **Historical Trends Tab**
   - [ ] Time range selector works (6h, 12h, 24h, etc.)
   - [ ] Power flow chart displays
   - [ ] SOC comparison chart shows both sources
   - [ ] Voltage, current, temperature charts render
   - [ ] Charts are interactive (hover tooltips work)

3. **Analytics Tab**
   - [ ] Summary cards show correct values
   - [ ] Energy production & consumption chart displays
   - [ ] Grid import/export chart displays
   - [ ] Daily breakdown table shows data
   - [ ] Excess energy is highlighted in red

4. **Cost Tracking Tab**
   - [ ] Net savings displays prominently
   - [ ] Energy flow breakdown is clear
   - [ ] Cost calculation details are shown
   - [ ] Rate input fields work and update calculations
   - [ ] Period selector works

5. **Predictions Tab**
   - [ ] Current SOC displays
   - [ ] Min/max predicted SOC shows
   - [ ] SOC forecast chart renders
   - [ ] Alerts for low/high SOC appear when appropriate
   - [ ] Prediction details table displays

6. **Excess Energy Tab** ‚ö° **CRITICAL**
   - [ ] Excess energy alert banner is prominent
   - [ ] Current status cards display
   - [ ] Load opportunities are listed
   - [ ] Immediate actions are clearly marked
   - [ ] Excess energy time series chart displays
   - [ ] Hourly patterns bar chart shows best hours
   - [ ] Recommended actions are prioritized
   - [ ] Optimal load hours table displays

---

#### Step 4.2: Browser Console Checks
**What to check:**
- No JavaScript errors
- API calls succeed
- Data is fetched properly

**Steps:**
1. Open browser DevTools (F12)
2. Go to Console tab
3. Navigate through all dashboard tabs
4. Look for:
   - [ ] No red error messages
   - [ ] No 404 or 500 API errors
   - [ ] No CORS issues
   - [ ] No React warnings

4. Go to Network tab
5. Filter by "Fetch/XHR"
6. Check each API call:
   - [ ] Returns 200 status
   - [ ] Response time < 2 seconds
   - [ ] Response data looks correct

---

### PHASE 5: Performance & Optimization (10-15 min)

#### Step 5.1: Query Performance
**What to test:**
- Endpoint response times
- Database query efficiency

**Test Cases:**
```bash
# Test response times for each endpoint
time curl -s "https://api.wildfireranch.us/energy/history?hours=24&limit=1000" > /dev/null
time curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=30" > /dev/null
time curl -s "https://api.wildfireranch.us/energy/analytics/excess?hours=168" > /dev/null
time curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=2025-09-01&end_date=2025-10-12" > /dev/null

# All should complete in < 2 seconds
```

**Database Performance:**
```sql
-- Check query plans for slow queries
EXPLAIN ANALYZE
SELECT
    DATE(created_at) as date,
    SUM(pv_power) / 60000.0 as total_solar_kwh
FROM solark.plant_flow
WHERE DATE(created_at) >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(created_at);

-- Check index usage
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname IN ('solark', 'victron')
ORDER BY idx_scan DESC;
```

**Questions to Answer:**
- [ ] Are all endpoints responding in < 2 seconds?
- [ ] Are database indexes being used effectively?
- [ ] Are there any slow queries that need optimization?
- [ ] Is TimescaleDB providing performance benefits?

---

### PHASE 6: Data Integrity & Edge Cases (10-15 min)

#### Step 6.1: Edge Case Testing
**What to test:**
- Handling of zero/null values
- Behavior during nighttime (no solar)
- Behavior during high load events
- Behavior during battery full/empty

**Test Cases:**
```bash
# Test 1: Query future dates (should return no data)
FUTURE_DATE=$(date -d '7 days' +%Y-%m-%d)
curl -s "https://api.wildfireranch.us/energy/analytics/cost?start_date=$FUTURE_DATE&end_date=$FUTURE_DATE" | jq '.status'

# Test 2: Query with invalid parameters
curl -s "https://api.wildfireranch.us/energy/history?hours=99999" | jq '.status'
curl -s "https://api.wildfireranch.us/energy/analytics/daily?days=-5" | jq '.status'

# Test 3: Check nighttime data (no solar)
SELECT
    created_at,
    pv_power,
    load_power,
    soc
FROM solark.plant_flow
WHERE EXTRACT(HOUR FROM created_at) BETWEEN 22 AND 6
ORDER BY created_at DESC
LIMIT 10;

# Test 4: Check division by zero handling
SELECT
    COUNT(*) FILTER (WHERE total_solar_kwh = 0) as days_with_zero_solar
FROM (
    SELECT
        DATE(created_at) as date,
        SUM(pv_power) / 60000.0 as total_solar_kwh
    FROM solark.plant_flow
    GROUP BY DATE(created_at)
) subquery;
```

**Validation Checklist:**
- [ ] Invalid parameters return proper error messages
- [ ] Nighttime data (zero solar) doesn't break calculations
- [ ] Division by zero is handled gracefully
- [ ] NULL values don't cause crashes
- [ ] Empty result sets return proper "no_data" status

---

### PHASE 7: Reporting & Documentation (10 min)

#### Step 7.1: Document Findings
**Create a validation report with:**

1. **Database Health Summary**
   - Total records in solark.plant_flow
   - Oldest and newest record timestamps
   - Data collection frequency
   - Victron schema status
   - Any critical gaps or issues

2. **Endpoint Performance Summary**
   - Response times for each endpoint
   - Success/failure rates
   - Any errors encountered
   - Recommendations for optimization

3. **Data Quality Summary**
   - Percentage of NULL values
   - Data range validation results
   - SOC comparison accuracy (Victron vs SolArk)
   - Anomalies detected

4. **Action Items**
   - [ ] Issues that need immediate fixing
   - [ ] Optimizations to implement
   - [ ] Missing features to add
   - [ ] Configuration changes needed

---

## üéØ Success Criteria

**Database:**
- [ ] SolArk data is being collected continuously (< 5 min gaps)
- [ ] Victron schema exists and is collecting data (or plan to initialize it)
- [ ] TimescaleDB hypertables are configured
- [ ] No excessive NULL values (< 5%)

**Endpoints:**
- [ ] All 6 new V1.7 endpoints return 200 OK
- [ ] Response times < 2 seconds for all queries
- [ ] Calculations are mathematically correct
- [ ] Data matches database queries

**Frontend:**
- [ ] All 6 tabs load without errors
- [ ] Charts render properly
- [ ] Data displays accurately
- [ ] No console errors

**Data Quality:**
- [ ] Values are within realistic ranges
- [ ] Excess energy calculation is correct
- [ ] Load opportunities make sense
- [ ] Predictions are reasonable

---

## üö® Red Flags to Watch For

1. **Database Issues:**
   - Large gaps in data collection (> 30 min)
   - No data in last 24 hours
   - Excessive NULL values (> 10%)
   - Flatlined data (same value repeated 100+ times)

2. **Endpoint Issues:**
   - Response times > 5 seconds
   - Frequent 500 errors
   - Incorrect calculations (excess energy negative, SOC > 100%, etc.)
   - Missing data in responses

3. **Poller Issues:**
   - Victron poller not running or unhealthy
   - SolArk data not updating
   - Rate limit violations (> 50 requests/hour for Victron)
   - Consecutive failures > 5

4. **Data Quality Issues:**
   - SOC difference between Victron and SolArk > 10%
   - Solar power at night (> 100W between 10pm-6am)
   - Load power = 0 for extended periods
   - Battery charging when SOC = 100% for hours

---

## üìù Output Format

**Create a validation report in markdown format:**

```markdown
# V1.7 Energy Analytics Dashboard - Validation Report
Date: 2025-10-13
Validator: [Your Name]

## Executive Summary
[2-3 sentences on overall health]

## Database Health
- Total SolArk records: X
- Data range: [oldest] to [newest]
- Victron status: [initialized/not initialized]
- Critical issues: [list or "None"]

## Endpoint Performance
| Endpoint | Status | Response Time | Issues |
|----------|--------|---------------|--------|
| /energy/history | ‚úÖ | 0.5s | None |
| /energy/analytics/daily | ‚úÖ | 1.2s | None |
| ... | ... | ... | ... |

## Data Quality
- NULL values: X%
- Data gaps: [description]
- Calculation accuracy: [verified/issues found]

## Action Items
1. [High Priority] Fix X
2. [Medium Priority] Optimize Y
3. [Low Priority] Consider Z

## Recommendations
[Your recommendations for improvements]
```

---

**Ready to validate! Start with Phase 1 and work through systematically. Good luck! üöÄ**
