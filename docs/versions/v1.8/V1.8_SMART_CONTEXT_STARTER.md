# V1.8 Smart Context Loading - Quick Start

**Version:** 1.8.0
**Priority:** High (40% cost reduction)
**Estimated Time:** 2-3 weeks
**Status:** Ready to implement

---

## ðŸŽ¯ What We're Building

Implement intelligent context loading with Redis caching to **reduce OpenAI costs by 40%** and **speed up responses**.

### The Problem

Currently, every query loads **all 24KB of context** (system specs + KB documents) regardless of whether it's needed:

- User asks: "What's my battery level?"
- System loads: Full system specs + all KB docs + conversation history
- Tokens used: 5k-8k tokens (~$0.025-$0.040)
- What's actually needed: System specs + battery tool (2k-3k tokens)

**Result:** Wasting $0.015-$0.025 per query on unnecessary context

### The Solution

Load **only relevant context** based on query type and **cache it** for 5 minutes:

```
Query: "What's my battery level?"
â†“
Classifier: SYSTEM query
â†“
Load: System specs only (no KB docs)
â†“
Cache: Store for 5 minutes
â†“
Tokens: 2k-3k (40% reduction!)
â†“
Next same query: Instant from cache
```

---

## ðŸ“Š Expected Impact

| Metric | Before (V1.7) | After (V1.8) | Improvement |
|--------|---------------|--------------|-------------|
| Tokens/query | 5k-8k | 3k-5k | **40% reduction** |
| Cost/query | $0.025-$0.040 | $0.015-$0.025 | **40% savings** |
| Context load time | N/A | <200ms (cached) | **Much faster** |
| Monthly OpenAI bill | $80-100 | $48-60 | **Save $32-40/mo** |

---

## ðŸš€ Implementation Prompt for Claude Code

**Copy and paste this to Claude Code to start:**

---

### Task: Implement V1.8 Smart Context Loading

**Goal:** Reduce token usage by 40% and improve response times by intelligently loading only relevant context with Redis caching.

**Reference Documentation:**
- **Full Implementation Guide:** [PROMPT_V2.0_SMART_CONTEXT.md](PROMPT_V2.0_SMART_CONTEXT.md) - Read this first for complete architecture
- **V2.0 Roadmap:** [docs/V2.0_ROADMAP.md](docs/V2.0_ROADMAP.md) - Section #2: Smart Context Loading
- **Current System:** V1.7 with full context loading in all agents

**What to implement:**

### Phase 1: Core Services (Week 1)

1. **Create ContextManager service** (`railway/src/services/context_manager.py`)
   - Query classifier (system, research, planning, general)
   - Smart context loading based on query type
   - Token budget enforcement (default: 3000 tokens)
   - Returns ContextBundle with metadata

2. **Create Redis client** (`railway/src/services/redis_client.py`)
   - Connection pooling with retry logic
   - Get/set/delete operations
   - 5-minute TTL for cached context
   - Graceful fallback if Redis unavailable

3. **Add configuration** (`railway/src/config/context_config.py`)
   - Token budgets per query type
   - Cache settings (TTL, enabled flag)
   - KB search limits
   - Fallback behavior

**Query Classification Rules:**
```python
SYSTEM:   "what's my battery", "current status", "solar production now"
          â†’ Load: System specs only (~2k tokens)

RESEARCH: "best practices", "latest trends", "should I upgrade"
          â†’ Load: System specs + KB docs + web search (~4k tokens)

PLANNING: "plan next week", "optimize schedule", "forecast"
          â†’ Load: System specs + historical data (~3.5k tokens)

GENERAL:  "hello", "thank you", greetings
          â†’ Load: Minimal context (~1k tokens)
```

### Phase 2: Agent Integration (Week 2)

4. **Update all agents to use ContextManager:**
   - âœ… Solar Controller (`railway/src/agents/solar_controller.py`)
   - âœ… Energy Orchestrator (`railway/src/agents/energy_orchestrator.py`)
   - âœ… Research Agent (`railway/src/agents/research_agent.py`)
   - âœ… Manager (`railway/src/agents/manager.py`)

**Before:**
```python
def create_energy_crew(query: str, conversation_context: str = None):
    full_context = get_context_files()  # Loads everything!
```

**After:**
```python
def create_energy_crew(query: str, user_id: str = None):
    context_mgr = ContextManager()
    context_bundle = context_mgr.get_relevant_context(
        query=query,
        user_id=user_id,
        max_tokens=3000
    )
    # Uses only relevant context
```

5. **Enhance API endpoint** (`railway/src/api/main.py`)
   - Add user_id to AskRequest model
   - Use ContextManager in /ask endpoint
   - Return context metadata in response:
     - `context_tokens: int`
     - `cache_hit: bool`
     - `query_type: str`

### Phase 3: Testing & Deployment (Week 3)

6. **Write comprehensive tests** (`railway/tests/test_context_manager.py`)
   - Query classification accuracy
   - Context loading logic
   - Redis caching behavior
   - Token budget enforcement
   - Error handling

7. **Add Railway Redis:**
   ```bash
   # In Railway dashboard, add Redis service
   # Set environment variable:
   REDIS_URL=${{Redis.REDIS_URL}}
   CONTEXT_MAX_TOKENS=3000
   CONTEXT_CACHE_ENABLED=true
   ```

8. **Deploy and monitor:**
   - Deploy to staging first
   - A/B test (compare token usage)
   - Monitor metrics
   - Full production rollout

**Success Criteria:**
- [ ] Average tokens per query reduced to 3k-5k (from 5k-8k)
- [ ] Cache hit rate >60% for repeated queries
- [ ] Context load time <200ms for cache hits
- [ ] All tests passing
- [ ] Agent accuracy maintained or improved
- [ ] Monthly OpenAI costs reduced by ~40%

**Files to create:**
```
railway/src/services/
â”œâ”€â”€ context_manager.py       (NEW)
â”œâ”€â”€ redis_client.py          (NEW)
â””â”€â”€ context_classifier.py    (NEW)

railway/src/config/
â””â”€â”€ context_config.py        (NEW)

railway/tests/
â””â”€â”€ test_context_manager.py  (NEW)
```

**Files to modify:**
```
railway/src/agents/solar_controller.py
railway/src/agents/energy_orchestrator.py
railway/src/agents/research_agent.py
railway/src/agents/manager.py
railway/src/api/main.py
railway/requirements.txt  (add: redis>=5.0.0)
railway/.env
```

**Environment variables to add:**
```bash
# Redis (Railway auto-provides)
REDIS_URL=${{Redis.REDIS_URL}}

# Context configuration
CONTEXT_MAX_TOKENS=3000
CONTEXT_CACHE_TTL=300
CONTEXT_CACHE_ENABLED=true
KB_MIN_SIMILARITY=0.3
```

**Testing commands:**
```bash
# Test system query (should use ~2500 tokens)
curl -X POST https://api.wildfireranch.us/ask \
  -H "Content-Type: application/json" \
  -d '{"message": "What is my battery level?", "user_id": "test"}'

# Check response includes:
# - context_tokens: ~2500 (down from 5k-8k!)
# - cache_hit: false (first request)
# - query_type: "system"

# Same query again (should hit cache)
curl -X POST https://api.wildfireranch.us/ask \
  -H "Content-Type: application/json" \
  -d '{"message": "What is my battery level?", "user_id": "test"}'

# Check response includes:
# - cache_hit: true (cached!)
# - Much faster response time
```

**Monitoring:**
Track these metrics in Railway logs:
- Average tokens per query (should drop 40%)
- Cache hit rate (target: >60%)
- Context load time (target: <200ms for hits)
- Redis connection health

**Reference the full implementation guide for:**
- Detailed code examples
- ContextBundle data structure
- Query classification logic
- Error handling patterns
- Edge cases to handle
- Performance optimization tips

See [PROMPT_V2.0_SMART_CONTEXT.md](PROMPT_V2.0_SMART_CONTEXT.md) for complete architecture and implementation details.

---

**Ready to start? Let's reduce those costs! ðŸ’°**

