# CommandCenter V2.1 ML Architecture
## Prefect Orchestration + Machine Learning Integration

**Version:** 2.1.0 (Planning Document)  
**Date:** 2025-10-17  
**Status:** Architecture Design - Implementation Target: Post V2.0  
**Dependencies:** V1.9 User Preferences, V2.0 Automation Engine

---

## 🎯 Executive Summary

V2.1 adds **machine learning intelligence** to CommandCenter's automation engine, transforming it from rule-based to predictive. The system will learn from historical patterns and forecast future energy states to make proactive decisions.

### Key Capabilities
- **Solar Production Forecasting** - Predict next 24-72 hours of solar generation
- **Load Prediction** - Forecast energy consumption patterns
- **Battery State Projection** - Simulate future battery SOC under various scenarios
- **Intelligent Scheduling** - ML-optimized miner and HVAC automation
- **Continuous Learning** - Models retrain nightly with new data

### Technology Stack
- **Orchestration:** Prefect 2.0+ (workflow management)
- **Time-Series ML:** Prophet by Meta (solar/load forecasting)
- **Decision ML:** XGBoost (classification and optimization)
- **Model Management:** MLflow (versioning and tracking)
- **Storage:** PostgreSQL + TimescaleDB (existing)

---

## 🏗️ System Architecture

### High-Level Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    DATA COLLECTION                          │
│                     (V1.5 - Existing)                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Victron Poller ───────┐                                   │
│  SolArk Integration ───┼──> PostgreSQL/TimescaleDB         │
│  Weather API ──────────┤    (telemetry storage)            │
│  Shelly Devices ───────┘                                   │
│                                                             │
└───────────────────┬─────────────────────────────────────────┘
                    │
                    │ All components read from same DB
                    │
         ┌──────────┴──────────┬──────────────┬──────────────┐
         │                     │              │              │
         ▼                     ▼              ▼              ▼
┌─────────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│  AGENTS (V1.9)  │  │ PREFECT V2.0 │  │ PREFECT V2.1 │  │  DASHBOARD   │
│  Advisory       │  │ Rules Engine │  │ ML Engine    │  │  (Vercel)    │
├─────────────────┤  ├──────────────┤  ├──────────────┤  ├──────────────┤
│ • Read DB       │  │ • Read DB    │  │ • Read DB    │  │ • Read DB    │
│ • Give advice   │  │ • Evaluate   │  │ • Train ML   │  │ • Display    │
│ • Explain logic │  │   rules      │  │ • Predict    │  │ • Visualize  │
│                 │  │ • Execute    │  │ • Optimize   │  │              │
│                 │  │   actions    │  │ • Execute    │  │              │
└─────────────────┘  └──────┬───────┘  └──────┬───────┘  └──────────────┘
                            │                  │
                            └────────┬─────────┘
                                     │
                                     ▼
                            ┌──────────────────┐
                            │  Hardware APIs   │
                            │  (Shelly, etc.)  │
                            └──────────────────┘
```

### Data Flow Detail

```
┌─────────────────────────────────────────────────────────────┐
│                   PHASE 1: DATA COLLECTION                  │
│                   (Continuous - Every 30s)                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Pollers → PostgreSQL                                       │
│  ├─ victron.telemetry (voltage, current, temp, SOC)        │
│  ├─ solark.plant_flow (solar_power, load_power, grid)      │
│  ├─ weather_data (cloud_cover, temperature, forecast)      │
│  └─ automation_logs (actions_taken, outcomes)              │
│                                                             │
└───────────────────┬─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│              PHASE 2: FEATURE ENGINEERING                   │
│                   (Daily - 1:00 AM)                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  @flow prepare_training_data()                              │
│  ├─ Extract: Last 90 days of telemetry                      │
│  ├─ Engineer: Time features, rolling windows, lags          │
│  ├─ Aggregate: Hourly, daily summaries                      │
│  └─ Store: ml_features table                                │
│                                                             │
└───────────────────┬─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│                PHASE 3: MODEL TRAINING                      │
│                   (Daily - 2:00 AM)                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  @flow train_all_models()                                   │
│  │                                                          │
│  ├─ @task train_solar_model()        (Prophet)             │
│  │   ├─ Features: time, weather, historical patterns       │
│  │   ├─ Target: solar_power (kW)                           │
│  │   ├─ Evaluate: RMSE, MAE, R²                            │
│  │   └─ Output: solar_model_v20251017.pkl                  │
│  │                                                          │
│  ├─ @task train_load_model()         (Prophet)             │
│  │   ├─ Features: time, day_of_week, historical            │
│  │   ├─ Target: load_power (kW)                            │
│  │   └─ Output: load_model_v20251017.pkl                   │
│  │                                                          │
│  ├─ @task train_battery_health()     (XGBoost)             │
│  │   ├─ Features: cycles, voltage_range, temperature       │
│  │   ├─ Target: capacity_degradation (%)                   │
│  │   └─ Output: battery_health_v20251017.pkl               │
│  │                                                          │
│  └─ @task register_models()          (MLflow)              │
│      ├─ Log metrics, parameters                            │
│      ├─ Compare to production models                       │
│      └─ Promote if better (A/B versioning)                 │
│                                                             │
└───────────────────┬─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│             PHASE 4: PREDICTION GENERATION                  │
│                   (Hourly - Every 1h)                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  @flow generate_predictions()                               │
│  │                                                          │
│  ├─ Load: Production models from MLflow                     │
│  ├─ Get: Current weather forecast (24h)                     │
│  │                                                          │
│  ├─ @task predict_solar_24h()                               │
│  │   ├─ Input: weather_forecast, time_features             │
│  │   ├─ Output: 24 hourly predictions                      │
│  │   └─ Store: ml_predictions table                        │
│  │                                                          │
│  ├─ @task predict_load_24h()                                │
│  │   ├─ Input: time_features, historical_patterns          │
│  │   ├─ Output: 24 hourly predictions                      │
│  │   └─ Store: ml_predictions table                        │
│  │                                                          │
│  └─ Cache: Redis (for fast API access)                      │
│                                                             │
└───────────────────┬─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│            PHASE 5: INTELLIGENT AUTOMATION                  │
│                   (Every 5 min - Continuous)                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  @flow ml_powered_automation()                              │
│  │                                                          │
│  ├─ Get: Current battery state                              │
│  ├─ Get: ML predictions from cache (fast!)                  │
│  │                                                          │
│  ├─ @task simulate_battery_state()                          │
│  │   ├─ Input: current_soc, solar_forecast, load_forecast  │
│  │   ├─ Simulate: Next 6 hours battery state               │
│  │   └─ Output: expected_soc_at_future_times               │
│  │                                                          │
│  ├─ @task optimize_miner_schedule()                         │
│  │   ├─ Consider: Predicted battery states                 │
│  │   ├─ Consider: User preferences (priorities)            │
│  │   ├─ Optimize: When to run which miners                 │
│  │   └─ Output: recommended_actions                        │
│  │                                                          │
│  ├─ Safety validation (always!)                             │
│  └─ Execute: If safe, implement actions                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 🧠 Machine Learning Models

### Model 1: Solar Production Forecasting

**Purpose:** Predict solar generation for next 24-72 hours

**Algorithm:** Prophet (Facebook's time-series forecasting)

**Why Prophet?**
- Built specifically for time-series with strong seasonal patterns
- Handles missing data gracefully
- Automatically detects daily/weekly/yearly seasonality
- Easy to add weather as external regressors
- Robust to outliers
- Provides uncertainty intervals (confidence bounds)

**Implementation:**
```python
from prophet import Prophet
import pandas as pd
import pickle

@task(retries=2)
def train_solar_model(training_data: pd.DataFrame) -> dict:
    """
    Train Prophet model to forecast solar production.
    
    Features:
    - timestamp (ds)
    - historical solar production (y)
    - cloud_cover (regressor)
    - temperature (regressor)
    - day_of_year (implicit via timestamp)
    
    Returns:
    - model: Trained Prophet model
    - metrics: RMSE, MAE, R²
    - version: Model version string
    """
    
    # Prepare data in Prophet format
    df = training_data[['timestamp', 'solar_power']].rename(
        columns={'timestamp': 'ds', 'solar_power': 'y'}
    )
    
    # Add weather regressors
    df['cloud_cover'] = training_data['cloud_cover']
    df['temperature'] = training_data['temperature']
    
    # Initialize Prophet with seasonality
    model = Prophet(
        daily_seasonality=True,      # Solar varies by time of day
        weekly_seasonality=True,     # Different on weekends?
        yearly_seasonality=True,     # Seasons affect solar angle
        changepoint_prior_scale=0.05 # Adaptability to trend changes
    )
    
    # Add weather impact
    model.add_regressor('cloud_cover', prior_scale=0.5)
    model.add_regressor('temperature', prior_scale=0.1)
    
    # Train model
    model.fit(df)
    
    # Evaluate on holdout set (last 7 days)
    train_df = df[:-168]  # All but last 7 days (168 hours)
    test_df = df[-168:]
    
    model_eval = Prophet()
    model_eval.fit(train_df)
    
    forecast = model_eval.predict(test_df[['ds', 'cloud_cover', 'temperature']])
    
    # Calculate metrics
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    rmse = mean_squared_error(test_df['y'], forecast['yhat'], squared=False)
    mae = mean_absolute_error(test_df['y'], forecast['yhat'])
    r2 = r2_score(test_df['y'], forecast['yhat'])
    
    # Save model
    version = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_path = f"models/solar_production_v{version}.pkl"
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    return {
        'model': model,
        'version': version,
        'metrics': {
            'rmse': float(rmse),
            'mae': float(mae),
            'r2': float(r2)
        },
        'model_path': model_path,
        'training_samples': len(df),
        'training_date': datetime.now().isoformat()
    }

@task
def predict_solar_24h(model, weather_forecast: pd.DataFrame) -> list:
    """
    Generate 24-hour solar production forecast.
    
    Args:
        model: Trained Prophet model
        weather_forecast: Next 24h weather (cloud_cover, temp)
    
    Returns:
        List of predictions with confidence intervals
    """
    
    # Create future dataframe
    future = pd.DataFrame({
        'ds': pd.date_range(
            start=datetime.now(),
            periods=24,
            freq='H'
        ),
        'cloud_cover': weather_forecast['cloud_cover'].values,
        'temperature': weather_forecast['temperature'].values
    })
    
    # Generate predictions
    forecast = model.predict(future)
    
    # Format predictions
    predictions = []
    for idx, row in forecast.iterrows():
        predictions.append({
            'timestamp': row['ds'].isoformat(),
            'solar_predicted_kw': round(row['yhat'], 2),
            'confidence_lower_kw': round(row['yhat_lower'], 2),
            'confidence_upper_kw': round(row['yhat_upper'], 2),
            'confidence_width': round(row['yhat_upper'] - row['yhat_lower'], 2)
        })
    
    return predictions
```

**Expected Performance:**
- **Accuracy:** 85-90% (clear days), 70-80% (variable weather)
- **RMSE:** < 2 kW on 14.6 kW max system
- **Inference Time:** < 100ms for 24h forecast

---

### Model 2: Load Prediction

**Purpose:** Forecast energy consumption patterns

**Algorithm:** Prophet (same as solar, different patterns)

**Why Prophet?**
- Handles daily usage patterns (peaks at certain hours)
- Captures weekly patterns (different on weekends)
- Adapts to seasonal changes (HVAC usage)

**Implementation:**
```python
@task
def train_load_model(training_data: pd.DataFrame) -> dict:
    """
    Train model to predict household load consumption.
    
    Features:
    - timestamp (ds)
    - historical load (y)
    - temperature (affects HVAC)
    - day_of_week (weekday vs weekend patterns)
    """
    
    df = training_data[['timestamp', 'load_power']].rename(
        columns={'timestamp': 'ds', 'load_power': 'y'}
    )
    
    # Add regressors
    df['temperature'] = training_data['temperature']
    df['is_weekend'] = training_data['timestamp'].dt.dayofweek.isin([5, 6])
    
    model = Prophet(
        daily_seasonality=True,
        weekly_seasonality=True,
        yearly_seasonality=False  # Load less seasonal than solar
    )
    
    model.add_regressor('temperature', prior_scale=0.3)
    model.add_regressor('is_weekend', prior_scale=0.2)
    
    model.fit(df)
    
    # Similar evaluation and saving as solar model
    # ... (metrics, version, path)
    
    return {
        'model': model,
        'version': version,
        'metrics': metrics
    }
```

**Expected Performance:**
- **Accuracy:** 80-85% (predictable usage)
- **RMSE:** < 1 kW on typical 5-10 kW load
- **Handles Surprises:** Lower accuracy on unexpected high loads

---

### Model 3: Battery Health Tracking

**Purpose:** Monitor battery degradation and predict capacity loss

**Algorithm:** XGBoost (regression)

**Why XGBoost?**
- Better for complex non-linear relationships
- Handles multiple interacting factors
- Feature importance helps identify degradation causes
- Fast inference

**Implementation:**
```python
import xgboost as xgb
from sklearn.metrics import mean_absolute_error

@task
def train_battery_health_model(training_data: pd.DataFrame) -> dict:
    """
    Train model to predict battery capacity degradation.
    
    Features:
    - total_cycles (charge/discharge cycles)
    - deep_cycles (cycles below 20% SOC)
    - voltage_range (max voltage - min voltage over period)
    - average_temperature (battery temp)
    - high_temp_hours (time spent > 30°C)
    - charge_rate_avg (average C-rate when charging)
    - age_days (battery age)
    
    Target:
    - capacity_loss_pct (% of original capacity lost)
    """
    
    # Prepare features
    X = training_data[[
        'total_cycles',
        'deep_cycles',
        'voltage_range',
        'average_temperature',
        'high_temp_hours',
        'charge_rate_avg',
        'age_days'
    ]]
    
    # Target (measured capacity loss)
    y = training_data['capacity_loss_pct']
    
    # Train/test split (last 30 days for testing)
    split_idx = -30
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Train XGBoost
    model = xgb.XGBRegressor(
        max_depth=6,
        learning_rate=0.1,
        n_estimators=100,
        objective='reg:squarederror',
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    # Evaluate
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    
    # Feature importance
    importance = dict(zip(X.columns, model.feature_importances_))
    
    # Save model
    version = datetime.now().strftime('%Y%m%d')
    model_path = f"models/battery_health_v{version}.json"
    model.save_model(model_path)
    
    return {
        'model': model,
        'version': version,
        'metrics': {'mae': float(mae)},
        'feature_importance': importance,
        'model_path': model_path
    }

@task
def predict_battery_health(model, current_battery_stats: dict) -> dict:
    """
    Predict current battery health and future degradation.
    
    Returns:
    - current_capacity_pct: Estimated current capacity
    - predicted_eol_date: When capacity reaches 80%
    - health_score: 0-100 battery health score
    - recommendations: Actions to slow degradation
    """
    
    import pandas as pd
    
    # Prepare features
    features = pd.DataFrame([{
        'total_cycles': current_battery_stats['cycles'],
        'deep_cycles': current_battery_stats['deep_cycles'],
        'voltage_range': current_battery_stats['voltage_range'],
        'average_temperature': current_battery_stats['avg_temp'],
        'high_temp_hours': current_battery_stats['high_temp_hours'],
        'charge_rate_avg': current_battery_stats['avg_charge_rate'],
        'age_days': current_battery_stats['age_days']
    }])
    
    # Predict capacity loss
    capacity_loss = model.predict(features)[0]
    current_capacity_pct = 100 - capacity_loss
    
    # Project to 80% capacity (end of life)
    degradation_rate = capacity_loss / current_battery_stats['age_days']
    days_to_eol = (20 - capacity_loss) / degradation_rate if degradation_rate > 0 else 9999
    eol_date = datetime.now() + timedelta(days=int(days_to_eol))
    
    # Calculate health score
    health_score = current_capacity_pct * 0.6 + \
                   (100 - current_battery_stats['deep_cycles'] / 100) * 0.2 + \
                   (100 - current_battery_stats['high_temp_hours'] / 1000) * 0.2
    
    # Generate recommendations
    recommendations = []
    if current_battery_stats['deep_cycles'] > 100:
        recommendations.append("Reduce deep discharge cycles (keep SOC > 30%)")
    if current_battery_stats['high_temp_hours'] > 500:
        recommendations.append("Improve battery cooling (high temperature exposure)")
    if current_battery_stats['avg_charge_rate'] > 0.5:
        recommendations.append("Consider slower charging rates")
    
    return {
        'current_capacity_pct': round(current_capacity_pct, 1),
        'predicted_eol_date': eol_date.isoformat(),
        'days_until_eol': int(days_to_eol),
        'health_score': round(health_score, 1),
        'degradation_rate_pct_per_year': round(degradation_rate * 365, 2),
        'recommendations': recommendations
    }
```

**Expected Performance:**
- **Accuracy:** 70-85% (limited training data initially)
- **MAE:** < 3% capacity loss prediction
- **Improves Over Time:** More accurate as data accumulates

---

### Model 4: Intelligent Decision Model

**Purpose:** Learn optimal miner scheduling decisions

**Algorithm:** XGBoost (classification)

**Why XGBoost?**
- Binary classification: "Should start miner?" (yes/no)
- Learns from historical automation outcomes
- Handles complex decision boundaries

**Implementation:**
```python
@task
def train_miner_decision_model(training_data: pd.DataFrame) -> dict:
    """
    Train model to make intelligent miner start/stop decisions.
    
    Features:
    - battery_voltage (current)
    - battery_voltage_trend (rising/falling)
    - solar_production_current (kW)
    - solar_forecast_6h (predicted avg next 6h)
    - load_current (kW)
    - hour_of_day (0-23)
    - day_of_week (0-6)
    - temperature (affects HVAC load)
    - miner_priority (1-10)
    - time_since_last_run (hours)
    
    Target:
    - action_was_successful (1 if action led to good outcome, 0 if not)
    
    Good outcome = battery stayed > 40% SOC after action
    """
    
    X = training_data[[
        'battery_voltage',
        'battery_voltage_trend',
        'solar_production',
        'solar_forecast_6h',
        'load_current',
        'hour_of_day',
        'day_of_week',
        'temperature',
        'miner_priority',
        'time_since_last_run'
    ]]
    
    y = training_data['action_was_successful']
    
    # Train model
    model = xgb.XGBClassifier(
        max_depth=6,
        learning_rate=0.1,
        n_estimators=100,
        objective='binary:logistic'
    )
    
    model.fit(X, y)
    
    # Evaluate
    from sklearn.metrics import accuracy_score, precision_score, recall_score
    
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    
    return {
        'model': model,
        'version': version,
        'metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall)
        }
    }

@task
def predict_miner_action(model, current_state: dict) -> dict:
    """
    Decide whether to start/stop a specific miner.
    
    Returns:
    - action: "start" or "stop"
    - confidence: 0-1 probability
    - reasoning: Feature importances for this decision
    """
    
    import pandas as pd
    
    features = pd.DataFrame([current_state])
    
    # Get prediction and probability
    prediction = model.predict(features)[0]
    probability = model.predict_proba(features)[0]
    
    action = "start" if prediction == 1 else "stop"
    confidence = probability[1] if prediction == 1 else probability[0]
    
    return {
        'action': action,
        'confidence': float(confidence),
        'features_used': current_state
    }
```

---

## 🔄 Prefect Workflow Orchestration

### Daily Training Pipeline

```python
from prefect import flow, task
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule
import mlflow

@flow(name="daily-ml-training", log_prints=True)
def train_all_models_flow():
    """
    Master flow: Train all ML models nightly.
    
    Schedule: Daily at 2:00 AM
    Duration: ~30-60 minutes
    """
    
    logger.info("🚀 Starting nightly ML training pipeline")
    
    # Step 1: Prepare training data
    logger.info("📊 Step 1: Feature engineering")
    features = prepare_training_data()
    
    # Step 2: Train models in parallel
    logger.info("🧠 Step 2: Training models")
    
    solar_future = train_solar_model.submit(features)
    load_future = train_load_model.submit(features)
    battery_future = train_battery_health_model.submit(features)
    
    # Wait for completion
    solar_result = solar_future.result()
    load_result = load_future.result()
    battery_result = battery_future.result()
    
    # Step 3: Evaluate and register with MLflow
    logger.info("📈 Step 3: Model evaluation")
    
    solar_promoted = register_model_if_better(
        model_name="solar_production",
        new_model=solar_result,
        threshold={'rmse': 2.0}
    )
    
    load_promoted = register_model_if_better(
        model_name="load_prediction",
        new_model=load_result,
        threshold={'rmse': 1.0}
    )
    
    battery_promoted = register_model_if_better(
        model_name="battery_health",
        new_model=battery_result,
        threshold={'mae': 3.0}
    )
    
    # Summary
    results = {
        'solar_model_version': solar_result['version'],
        'solar_promoted': solar_promoted,
        'load_model_version': load_result['version'],
        'load_promoted': load_promoted,
        'battery_model_version': battery_result['version'],
        'battery_promoted': battery_promoted,
        'training_completed_at': datetime.now().isoformat()
    }
    
    logger.info(f"✅ Training complete: {results}")
    
    # Notify if models updated
    if any([solar_promoted, load_promoted, battery_promoted]):
        send_notification(
            "New ML models deployed",
            f"Updated models: {[k for k,v in results.items() if 'promoted' in k and v]}"
        )
    
    return results

@task
def prepare_training_data() -> pd.DataFrame:
    """
    Extract and engineer features from last 90 days of telemetry.
    
    Returns:
        Dataframe with engineered features ready for training
    """
    
    # Query database
    query = """
        SELECT 
            v.timestamp,
            v.battery_voltage,
            v.battery_current,
            v.battery_temperature,
            v.battery_soc,
            s.solar_power,
            s.load_power,
            s.grid_power,
            w.cloud_cover,
            w.temperature AS weather_temp
        FROM victron.telemetry v
        JOIN solark.plant_flow s ON DATE_TRUNC('minute', v.timestamp) = DATE_TRUNC('minute', s.timestamp)
        LEFT JOIN weather_data w ON DATE_TRUNC('hour', v.timestamp) = DATE_TRUNC('hour', w.timestamp)
        WHERE v.timestamp >= NOW() - INTERVAL '90 days'
        ORDER BY v.timestamp
    """
    
    df = pd.read_sql(query, db_engine)
    
    # Engineer features
    df = engineer_time_features(df)
    df = engineer_rolling_features(df)
    df = engineer_lag_features(df)
    
    logger.info(f"Prepared {len(df)} samples with {len(df.columns)} features")
    
    return df

@task
def engineer_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add time-based features"""
    
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['day_of_year'] = df['timestamp'].dt.dayofyear
    df['month'] = df['timestamp'].dt.month
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_daytime'] = df['hour'].between(6, 18).astype(int)
    
    return df

@task
def engineer_rolling_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add rolling window features"""
    
    # 24-hour moving averages
    df['solar_ma_24h'] = df['solar_power'].rolling(24).mean()
    df['load_ma_24h'] = df['load_power'].rolling(24).mean()
    df['voltage_ma_24h'] = df['battery_voltage'].rolling(24).mean()
    
    # Trends (difference from moving average)
    df['solar_trend'] = df['solar_power'] - df['solar_ma_24h']
    df['voltage_trend'] = df['battery_voltage'].diff()
    
    return df

@task
def engineer_lag_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add lagged features"""
    
    # Previous hour values
    df['solar_1h_ago'] = df['solar_power'].shift(1)
    df['voltage_1h_ago'] = df['battery_voltage'].shift(1)
    
    # Previous day same time
    df['solar_24h_ago'] = df['solar_power'].shift(24)
    
    return df

@task
def register_model_if_better(
    model_name: str,
    new_model: dict,
    threshold: dict
) -> bool:
    """
    Register new model with MLflow only if better than current production.
    
    Args:
        model_name: Name in MLflow registry
        new_model: Training results dict
        threshold: Metric thresholds for promotion
    
    Returns:
        True if model was promoted to production
    """
    
    import mlflow
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient()
    
    with mlflow.start_run():
        # Log new model
        mlflow.log_params({
            'training_date': new_model['training_date'],
            'training_samples': new_model['training_samples'],
            'version': new_model['version']
        })
        
        mlflow.log_metrics(new_model['metrics'])
        
        # Save model artifact
        mlflow.sklearn.log_model(new_model['model'], model_name)
        
        # Get current production model metrics
        try:
            prod_version = client.get_latest_versions(model_name, stages=["Production"])[0]
            prod_run = client.get_run(prod_version.run_id)
            prod_metrics = prod_run.data.metrics
        except:
            # No production model yet
            prod_metrics = {k: float('inf') for k in new_model['metrics'].keys()}
        
        # Compare (lower is better for RMSE/MAE)
        metric_key = list(threshold.keys())[0]
        new_value = new_model['metrics'][metric_key]
        prod_value = prod_metrics.get(metric_key, float('inf'))
        
        if new_value < prod_value and new_value < threshold[metric_key]:
            # New model is better!
            model_uri = f"runs:/{mlflow.active_run().info.run_id}/{model_name}"
            
            mlflow.register_model(model_uri, model_name)
            
            # Promote to production
            latest_version = client.get_latest_versions(model_name)[0].version
            client.transition_model_version_stage(
                name=model_name,
                version=latest_version,
                stage="Production"
            )
            
            logger.info(f"✅ {model_name} v{new_model['version']} promoted!")
            logger.info(f"   {metric_key}: {prod_value:.3f} → {new_value:.3f}")
            
            return True
        else:
            logger.info(f"⏸️ {model_name} v{new_model['version']} not promoted")
            logger.info(f"   {metric_key}: current {prod_value:.3f} vs new {new_value:.3f}")
            
            return False

# Deploy training flow
training_deployment = Deployment.build_from_flow(
    flow=train_all_models_flow,
    name="nightly-ml-training",
    schedule=CronSchedule(cron="0 2 * * *"),  # 2 AM daily
    work_pool_name="default"
)
```

### Hourly Prediction Pipeline

```python
@flow(name="hourly-predictions", log_prints=True)
def generate_predictions_flow():
    """
    Generate predictions for next 24 hours.
    
    Schedule: Every hour
    Duration: ~2-5 minutes
    """
    
    logger.info("🔮 Generating ML predictions")
    
    # Load production models from MLflow
    solar_model = load_production_model("solar_production")
    load_model = load_production_model("load_prediction")
    
    # Get weather forecast
    weather_forecast = fetch_weather_forecast(hours=24)
    
    # Generate predictions in parallel
    solar_future = predict_solar_24h.submit(solar_model, weather_forecast)
    load_future = predict_load_24h.submit(load_model)
    
    solar_predictions = solar_future.result()
    load_predictions = load_future.result()
    
    # Store predictions
    store_predictions_db(solar_predictions, load_predictions)
    
    # Cache for fast API access
    cache_predictions_redis(solar_predictions, load_predictions)
    
    logger.info(f"✅ Generated {len(solar_predictions)} predictions")
    
    return {
        'predictions_count': len(solar_predictions),
        'generated_at': datetime.now().isoformat()
    }

@task
def load_production_model(model_name: str):
    """Load latest production model from MLflow"""
    
    import mlflow
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient()
    
    # Get production version
    prod_versions = client.get_latest_versions(model_name, stages=["Production"])
    
    if not prod_versions:
        raise ValueError(f"No production model found for {model_name}")
    
    model_uri = f"models:/{model_name}/Production"
    model = mlflow.sklearn.load_model(model_uri)
    
    logger.info(f"Loaded {model_name} v{prod_versions[0].version}")
    
    return model

@task(retries=3, retry_delay_seconds=60)
def fetch_weather_forecast(hours: int = 24) -> pd.DataFrame:
    """Fetch weather forecast from API"""
    
    # Your weather API integration
    # Returns: cloud_cover, temperature for next N hours
    pass

@task
def store_predictions_db(solar_predictions: list, load_predictions: list):
    """Store predictions in PostgreSQL"""
    
    # Combine predictions
    combined = []
    for solar, load in zip(solar_predictions, load_predictions):
        combined.append({
            'timestamp': solar['timestamp'],
            'solar_predicted_kw': solar['solar_predicted_kw'],
            'solar_confidence_lower': solar['confidence_lower_kw'],
            'solar_confidence_upper': solar['confidence_upper_kw'],
            'load_predicted_kw': load['load_predicted_kw'],
            'load_confidence_lower': load['confidence_lower_kw'],
            'load_confidence_upper': load['confidence_upper_kw'],
            'generated_at': datetime.now()
        })
    
    # Bulk insert
    db.execute("""
        INSERT INTO ml_predictions 
        (timestamp, solar_predicted_kw, solar_confidence_lower, solar_confidence_upper,
         load_predicted_kw, load_confidence_lower, load_confidence_upper, generated_at)
        VALUES (...)
    """, combined)

@task
def cache_predictions_redis(solar_predictions: list, load_predictions: list):
    """Cache predictions in Redis for fast API access"""
    
    import redis
    import json
    
    redis_client = redis.Redis(host='localhost', port=6379)
    
    # Cache with 2-hour expiry
    redis_client.setex(
        'ml_predictions:solar_24h',
        7200,  # 2 hours
        json.dumps(solar_predictions)
    )
    
    redis_client.setex(
        'ml_predictions:load_24h',
        7200,
        json.dumps(load_predictions)
    )

# Deploy prediction flow
prediction_deployment = Deployment.build_from_flow(
    flow=generate_predictions_flow,
    name="hourly-predictions",
    schedule=CronSchedule(cron="0 * * * *"),  # Every hour
    work_pool_name="default"
)
```

### ML-Powered Automation (Every 5 min)

```python
@flow(name="ml-automation", log_prints=True)
def ml_powered_automation_flow():
    """
    Use ML predictions to make intelligent automation decisions.
    
    Schedule: Every 5 minutes
    Duration: < 1 minute
    """
    
    # Get current state
    battery_state = get_battery_state()
    
    # Get ML predictions from cache (fast!)
    predictions = get_cached_predictions()
    
    # Simulate battery state over next 6 hours
    battery_forecast = simulate_battery_state(
        current_soc=battery_state['soc'],
        current_voltage=battery_state['voltage'],
        solar_forecast=predictions['solar'][:6],
        load_forecast=predictions['load'][:6]
    )
    
    # Make intelligent decisions
    miner_decisions = optimize_miner_schedule(
        battery_forecast=battery_forecast,
        user_preferences=get_user_preferences()
    )
    
    hvac_decisions = optimize_hvac_control(
        battery_forecast=battery_forecast,
        zone_temps=get_zone_temperatures()
    )
    
    # Execute decisions (with safety checks)
    execute_if_safe(miner_decisions, hvac_decisions)
    
    return {
        'battery_forecast_6h': battery_forecast,
        'actions_taken': miner_decisions + hvac_decisions
    }

@task
def get_cached_predictions() -> dict:
    """Get predictions from Redis cache"""
    
    import redis
    import json
    
    redis_client = redis.Redis(host='localhost', port=6379)
    
    solar = json.loads(redis_client.get('ml_predictions:solar_24h'))
    load = json.loads(redis_client.get('ml_predictions:load_24h'))
    
    return {'solar': solar, 'load': load}

@task
def simulate_battery_state(
    current_soc: float,
    current_voltage: float,
    solar_forecast: list,
    load_forecast: list
) -> list:
    """
    Simulate battery SOC for next 6 hours.
    
    Simple energy balance model:
    - SOC_next = SOC_current + (solar - load) / battery_capacity
    
    Returns:
        List of predicted SOC values for each hour
    """
    
    BATTERY_CAPACITY_KWH = 10.0  # Your system capacity
    
    simulated_states = []
    current_soc_sim = current_soc
    
    for hour, (solar, load) in enumerate(zip(solar_forecast, load_forecast)):
        # Energy balance
        net_energy_kwh = (solar['solar_predicted_kw'] - load['load_predicted_kw'])
        
        # SOC change (%)
        soc_change = (net_energy_kwh / BATTERY_CAPACITY_KWH) * 100
        
        # Update SOC (clamp to 0-100%)
        current_soc_sim = max(0, min(100, current_soc_sim + soc_change))
        
        simulated_states.append({
            'hour': hour + 1,
            'timestamp': (datetime.now() + timedelta(hours=hour+1)).isoformat(),
            'predicted_soc': round(current_soc_sim, 1),
            'solar_kw': solar['solar_predicted_kw'],
            'load_kw': load['load_predicted_kw'],
            'net_kw': round(net_energy_kwh, 2)
        })
    
    return simulated_states

@task
def optimize_miner_schedule(
    battery_forecast: list,
    user_preferences: dict
) -> list:
    """
    Decide which miners to run based on battery forecast.
    
    Logic:
    - If battery stays > 60% for next 6h → Run all miners
    - If battery stays > 50% for next 6h → Run priority 1-2
    - If battery stays > 40% for next 6h → Run priority 1 only
    - Otherwise → Stop all miners
    """
    
    # Get minimum SOC over next 6 hours
    min_future_soc = min([h['predicted_soc'] for h in battery_forecast])
    
    # Get all miners sorted by priority
    miners = get_miner_profiles()  # From V1.9 database
    
    decisions = []
    
    for miner in miners:
        # Determine if we can safely run this miner
        should_run = False
        confidence = "high"
        
        if min_future_soc > 60 and miner['priority'] <= 10:
            should_run = True
            confidence = "high"
        elif min_future_soc > 50 and miner['priority'] <= 2:
            should_run = True
            confidence = "high"
        elif min_future_soc > 40 and miner['priority'] == 1:
            should_run = True
            confidence = "medium"
        
        # Check voltage threshold (from V1.9 preferences)
        current_voltage = get_battery_voltage()
        if current_voltage < miner['start_voltage']:
            should_run = False
            confidence = "high"
        
        decisions.append({
            'miner_id': miner['id'],
            'miner_name': miner['name'],
            'action': 'start' if should_run else 'stop',
            'confidence': confidence,
            'reason': f"Min future SOC: {min_future_soc}%, Priority: {miner['priority']}"
        })
    
    return decisions

# Deploy automation flow
automation_deployment = Deployment.build_from_flow(
    flow=ml_powered_automation_flow,
    name="ml-automation",
    schedule=CronSchedule(cron="*/5 * * * *"),  # Every 5 minutes
    work_pool_name="default"
)
```

---

## 📊 Database Schema Extensions

### ML Predictions Table

```sql
CREATE TABLE ml_predictions (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    
    -- Solar predictions
    solar_predicted_kw DECIMAL(6,2),
    solar_confidence_lower DECIMAL(6,2),
    solar_confidence_upper DECIMAL(6,2),
    
    -- Load predictions
    load_predicted_kw DECIMAL(6,2),
    load_confidence_lower DECIMAL(6,2),
    load_confidence_upper DECIMAL(6,2),
    
    -- Metadata
    model_version_solar TEXT,
    model_version_load TEXT,
    generated_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT unique_prediction_timestamp UNIQUE (timestamp)
);

-- Index for fast lookups
CREATE INDEX idx_ml_predictions_timestamp ON ml_predictions (timestamp DESC);

-- TimescaleDB hypertable
SELECT create_hypertable('ml_predictions', 'timestamp');
```

### ML Training History Table

```sql
CREATE TABLE ml_training_history (
    id BIGSERIAL PRIMARY KEY,
    model_name TEXT NOT NULL,
    version TEXT NOT NULL,
    
    -- Metrics
    metric_rmse DECIMAL(8,4),
    metric_mae DECIMAL(8,4),
    metric_r2 DECIMAL(6,4),
    metric_accuracy DECIMAL(6,4),
    
    -- Training details
    training_samples INT,
    training_duration_seconds INT,
    features_used JSONB,
    
    -- Promotion
    promoted_to_production BOOLEAN DEFAULT FALSE,
    promotion_date TIMESTAMPTZ,
    
    -- Metadata
    trained_at TIMESTAMPTZ DEFAULT NOW(),
    trained_by TEXT DEFAULT 'prefect',
    
    CONSTRAINT unique_model_version UNIQUE (model_name, version)
);

CREATE INDEX idx_ml_training_model ON ml_training_history (model_name, trained_at DESC);
```

### ML Feature Store Table

```sql
CREATE TABLE ml_features (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    
    -- Raw features
    battery_voltage DECIMAL(5,2),
    battery_current DECIMAL(6,2),
    battery_soc DECIMAL(5,2),
    solar_power DECIMAL(6,2),
    load_power DECIMAL(6,2),
    cloud_cover DECIMAL(5,2),
    temperature DECIMAL(4,1),
    
    -- Engineered features
    hour INT,
    day_of_week INT,
    is_weekend BOOLEAN,
    solar_ma_24h DECIMAL(6,2),
    voltage_trend DECIMAL(5,2),
    solar_1h_ago DECIMAL(6,2),
    
    -- Metadata
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Hypertable for efficient time-series storage
SELECT create_hypertable('ml_features', 'timestamp');

-- Retention policy (keep 1 year of features)
SELECT add_retention_policy('ml_features', INTERVAL '1 year');
```

---

## 🚀 Implementation Roadmap

### Phase 1: Foundation (V2.0 - Weeks 1-3)
**Focus:** Rule-based automation with Prefect

**Deliverables:**
- ✅ Install Prefect
- ✅ Create basic automation flows (battery monitor, miner control)
- ✅ Implement safety validation layer
- ✅ Add action logging
- ✅ Deploy to Railway

**No ML yet** - Just get Prefect working with rule-based logic

---

### Phase 2: Data Collection (V2.0 - Week 3)
**Focus:** Log data for ML training

**Deliverables:**
- ✅ Create ml_features table
- ✅ Background task: Engineer and store features daily
- ✅ Accumulate 30+ days of structured data
- ✅ Add automation_logs table with outcomes

**Goal:** Gather training data while running rule-based automation

---

### Phase 3: ML Models (V2.1 - Weeks 1-2)
**Focus:** Train initial models

**Deliverables:**
- ✅ Install ML stack (Prophet, XGBoost, MLflow)
- ✅ Create training pipeline flows
- ✅ Train solar production model
- ✅ Train load prediction model
- ✅ Deploy prediction generation flow

**Milestone:** First ML predictions available

---

### Phase 4: ML Integration (V2.1 - Weeks 2-3)
**Focus:** Use ML predictions in automation

**Deliverables:**
- ✅ Modify automation flows to read ML predictions
- ✅ Add battery state simulation
- ✅ Implement ML-powered decision logic
- ✅ A/B test: ML vs rules-based
- ✅ Dashboard showing ML predictions

**Milestone:** ML-powered automation live

---

### Phase 5: Continuous Learning (V2.1 - Week 3)
**Focus:** Automatic improvement

**Deliverables:**
- ✅ Nightly model retraining
- ✅ Automatic A/B testing
- ✅ Model performance monitoring
- ✅ Alerting for model degradation

**Milestone:** Self-improving system

---

### Phase 6: Advanced Models (V2.2+)
**Focus:** Additional ML capabilities

**Future Enhancements:**
- Battery health tracking model
- Miner decision learning model
- Weather impact models
- Cost optimization models

---

## 📈 Success Metrics

### Model Performance Targets

**Solar Production Model:**
- RMSE: < 2.0 kW (on 14.6 kW max system)
- MAE: < 1.5 kW
- R²: > 0.85
- Accuracy: 85-90% on clear days

**Load Prediction Model:**
- RMSE: < 1.0 kW (on typical 5-10 kW load)
- MAE: < 0.7 kW
- R²: > 0.80
- Accuracy: 80-85%

**Battery Health Model:**
- MAE: < 3% capacity loss prediction
- Accuracy: 70-85% (improves over time)

**Decision Model:**
- Accuracy: > 85% (did action lead to good outcome?)
- Precision: > 80% (avoid false positives)
- Recall: > 75% (catch opportunities)

### Business Metrics

**Energy Savings:**
- Baseline: Rule-based automation savings
- Target: +10-15% improvement with ML
- Measure: Monthly energy costs before/after

**Battery Longevity:**
- Baseline: Current degradation rate
- Target: -20% slower degradation
- Measure: Capacity loss over 6 months

**Miner Uptime:**
- Baseline: Current miner runtime
- Target: +15% runtime with same battery health
- Measure: Total miner hours per month

---

## 🔧 Technology Stack Summary

### Core ML Libraries
```bash
pip install prefect==2.14+         # Workflow orchestration
pip install prophet==1.1+          # Time-series forecasting
pip install xgboost==2.0+          # Gradient boosting
pip install mlflow==2.9+           # Model tracking
pip install scikit-learn==1.3+    # ML utilities
pip install pandas==2.1+           # Data manipulation
pip install numpy==1.26+           # Numerical computing
```

### Optional Enhancements
```bash
pip install optuna==3.5+           # Hyperparameter tuning
pip install shap==0.43+            # Model explainability
pip install plotly==5.18+          # Visualization
```

### Infrastructure
- **Database:** PostgreSQL 16 + TimescaleDB (existing)
- **Cache:** Redis 7+ (for prediction caching)
- **Model Storage:** MLflow tracking server
- **Deployment:** Railway (same as existing backend)

---

## 🎓 Learning Resources

### Prophet Documentation
- Official Docs: https://facebook.github.io/prophet/
- Quick Start: https://facebook.github.io/prophet/docs/quick_start.html
- Tutorial: Time series with multiple seasonality

### XGBoost Documentation
- Official Docs: https://xgboost.readthedocs.io/
- Python API: https://xgboost.readthedocs.io/en/stable/python/
- Tutorials: Classification and regression

### Prefect Documentation
- Official Docs: https://docs.prefect.io/
- Flows & Tasks: https://docs.prefect.io/concepts/flows/
- Deployments: https://docs.prefect.io/concepts/deployments/

### MLflow Documentation
- Official Docs: https://mlflow.org/docs/latest/
- Model Registry: https://mlflow.org/docs/latest/model-registry.html
- Tracking: https://mlflow.org/docs/latest/tracking.html

---

## ⚠️ Important Considerations

### 1. Data Quality is Critical
- **Garbage in, garbage out** - ML models are only as good as training data
- Ensure telemetry accuracy before training
- Monitor for sensor drift or failures
- Handle missing data appropriately

### 2. Start Simple, Iterate
- Begin with Prophet for forecasting (simplest)
- Add XGBoost for decisions (more complex)
- Don't over-engineer initially
- Validate each model before adding next

### 3. Human Oversight Required
- **ML suggestions, not commands** - Always allow manual override
- Monitor model decisions closely initially
- A/B test ML vs rules for 2-4 weeks
- Have rollback plan if ML performs poorly

### 4. Continuous Monitoring
- Track model performance metrics
- Alert if accuracy degrades
- Retrain models regularly (nightly)
- Archive old models for comparison

### 5. Explainability Matters
- Use SHAP or similar for feature importance
- Show users WHY model made decision
- Build trust through transparency
- Log reasoning with each prediction

---

## 📝 Next Steps After V1.9

### Before Starting V2.1 ML Work

1. **Complete V1.9** - User preferences system solid
2. **Deploy V2.0** - Rule-based Prefect automation working
3. **Collect Data** - Run V2.0 for 30+ days to gather training data
4. **Review This Document** - Refine plan based on V2.0 learnings

### When Ready for V2.1

1. **Set up MLflow** - Model tracking infrastructure
2. **Create Training Pipeline** - Start with solar model
3. **Validate Predictions** - Compare to actual data for 1 week
4. **Integrate Gradually** - Use predictions in automation slowly
5. **Monitor & Iterate** - Continuous improvement

---

## 🎯 Conclusion

This architecture provides a **complete roadmap** for adding machine learning intelligence to CommandCenter. By leveraging Prefect for orchestration and proven ML frameworks (Prophet, XGBoost), you'll transform the system from reactive to predictive while maintaining the solid foundation built in V1.9 and V2.0.

The key insight: **Prefect + ML is not a replacement for your rule-based logic - it's an enhancement.** Rules provide the safety boundaries, ML optimizes within those boundaries.

**Document Status:** Planning / Architecture Design  
**Last Updated:** 2025-10-17  
**Next Review:** After V2.0 deployment  
**Owner:** CommandCenter Development Team